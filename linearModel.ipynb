{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression, the general prediction formula for a linear model looks as follows:\n",
    "\n",
    "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\n",
    "\n",
    "Here, x[0] to x[p] denotes the features (in this example, the number of features is p)\n",
    "of a single data point, w and b are parameters of the model that are learned, and ŷ is\n",
    "the prediction the model makes. For a dataset with a single feature, this is:\n",
    "\n",
    "ŷ = w[0] * x[0] + b\n",
    "\n",
    "w[0] is the slope and b is the y-axis offset.\n",
    "\n",
    "For more features, w contains the\n",
    "slopes along each feature axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models for regression can be characterized as regression models for which the\n",
    "prediction is a line for a single feature, a plane when using two features, or a hyper‐\n",
    "plane in higher dimensions (that is, when using more features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression (aka ordinary least squares)\n",
    "Linear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\n",
    "ear method for regression. Linear regression finds the parameters w and b that mini‐\n",
    "mize the mean squared error between predictions and the true regression targets, y,\n",
    "on the training set. The mean squared error is the sum of the squared differences\n",
    "between the predictions and the true values. Linear regression has no parameters,\n",
    "which is a benefit, but it also has no way to control model complexity"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
